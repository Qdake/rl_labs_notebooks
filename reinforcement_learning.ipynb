{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toolbox\n",
    "# tool box and maze_plotter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "##########-action constants-###################\n",
    "N = 0\n",
    "S = 1\n",
    "E = 2\n",
    "W = 3\n",
    "NOOP = 4  \n",
    "\n",
    "\n",
    "def discreteProb(p):\n",
    "        # Draw a random number using probability table p (column vector)\n",
    "        # Suppose probabilities p=[p(1) ... p(n)] for the values [1:n] are given, sum(p)=1 \n",
    "        # and the components p(j) are nonnegative. \n",
    "        # To generate a random sample of size m from this distribution,\n",
    "        #imagine that the interval (0,1) is divided into intervals with the lengths p(1),...,p(n). \n",
    "        # Generate a uniform number rand, if this number falls in the jth interval given the discrete distribution,\n",
    "        # return the value j. Repeat m times.\n",
    "        r = np.random.random()\n",
    "        cumprob=np.hstack((np.zeros(1),p.cumsum()))\n",
    "        sample = -1\n",
    "        for j in range(p.size):\n",
    "            if (r>cumprob[j]) & (r<=cumprob[j+1]):\n",
    "                sample = j\n",
    "                break\n",
    "        return sample\n",
    "\n",
    "def softmax(Q,x,tau):\n",
    "    # Returns a soft-max probability distribution over actions\n",
    "    # Inputs :\n",
    "    # - Q : a Q-function represented as a nX times nU matrix\n",
    "    # - x : the state for which we want the soft-max distribution\n",
    "    # - tau : temperature parameter of the soft-max distribution\n",
    "    # Output :\n",
    "    # - p : probability of each action according to the soft-max distribution\n",
    "    \n",
    "    p = np.zeros((len(Q[x])))\n",
    "    sump = 0\n",
    "    for i in range(len(p)) :\n",
    "        p[i] = np.exp((Q[x,i]/tau).round(5))\n",
    "        sump += p[i]\n",
    "    \n",
    "    p = p/sump\n",
    "    \n",
    "    return p\n",
    "\n",
    "\n",
    "\n",
    "def compare(V,Q,pol): \n",
    "    # compares the state value V with the state-action value Q following policy pol\n",
    "    epsilon = 0.01 # precision of the comparison\n",
    "    sumval = np.zeros((V.size))\n",
    "    for i in range(V.size): # compute the difference between V and Q for each state\n",
    "        sumval[i] = abs(V[i] - Q[i,pol[i]])\n",
    "         \n",
    "    if np.max(sumval)<epsilon :\n",
    "        return True\n",
    "    else :\n",
    "        return False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning functions\n",
    "\n",
    "By contrast with dynamic programming functions, the reinforcement learning functions are applied when the MDP is unknown.\n",
    "More precisely, the state and action spaces are known, but the agent does not know the transition nor the reward function.\n",
    "\n",
    "In this notebook we focus on *model free* reinforcement learning, the model-based case is treated [here](mbrl.ipynb).\n",
    "\n",
    "## TD learning\n",
    "\n",
    "Given a state and an action spaces as well as a policy, TD(0) computes the state value of this policy based on the following equation: \n",
    "    $$V^{(t+1)}(x_t) = V^{(t)}(x_t) + \\alpha\\delta_t,$$\n",
    "    \n",
    "where $\\delta_t = r(x_t,u_t) + \\gamma V^{(t)}(x_{t+1})-V^{(t)}(x_t)$ is the TD error and $\\alpha$ is a parameter called \"learning rate\".</span>\n",
    "\n",
    "\n",
    "The code is provided below, so that you can take inspiration later on. The important part is the computation of $\\delta$, and the update of the values of $V$.\n",
    "\n",
    "Once you have understood the code, write in the cell just below the code to run it.\n",
    "\n",
    "Hint: to run TD learning, you need a policy as input. You can retreive such a policy by using the policy iteration method defined in the [dynamic_programming.ipynb](dynamic_programming.ipynb) notebook. Just import it, call it, and use the resulting policy as evaluated policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given state and action spaces and a policy, it computes the state value of this policy\n",
    "\n",
    "def TD(mdp, pol, nEpisodes=100000, nTimesteps=25, render=True):\n",
    "    V = np.zeros((mdp.observation_space.size)) #initial state value V\n",
    "    V_list = [V.copy()]\n",
    "    alpha = 0.1 #learning rate\n",
    "    mdp.timeout = 25 #sets timeout of an episode (maximum number of timesteps) - default set to 50\n",
    "    \n",
    "    if render:\n",
    "        mdp.new_render()\n",
    "    \n",
    "    for i in range(nEpisodes) : #for each episode\n",
    "        \n",
    "        # Draw an initial state randomly (if uniform is set to False, the state is drawn according to the P0 \n",
    "        #                                 distribution)\n",
    "        x = mdp.reset(uniform=True) \n",
    "        done = mdp.done()\n",
    "        while not done: #update episode at each timestep\n",
    "            # Show agent\n",
    "            if render:\n",
    "                mdp.render(V, pol)\n",
    "            \n",
    "            # Step forward following the MDP: x=current state, \n",
    "            #                                 pol[i]=agent's action according to policy pol, \n",
    "            #                                 r=reward gained after taking action pol[i], \n",
    "            #                                 done=tells whether the episode ended, \n",
    "            #                                 and info gives some info about the process\n",
    "            [y,r,done,info] = mdp.step(pol[x]) \n",
    "            \n",
    "            # Update the state value of x\n",
    "            if x in mdp.terminal_states:\n",
    "                V[x] = r\n",
    "            else:\n",
    "                delta = r+mdp.gamma*V[y]-V[x]\n",
    "                V[x] = V[x]+alpha*delta\n",
    "            \n",
    "            # Update agent's position (state)\n",
    "            x=y\n",
    "        \n",
    "        # After each episode, we save the computed state value V\n",
    "        V_list.append(V.copy()) \n",
    "    \n",
    "    if render :\n",
    "        mdp.render(V, pol)\n",
    "    \n",
    "    return V_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\\\\write here the code to call the above TD function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d68b790bbc40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'notebook'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolPI_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# V_list is the list of the V values of each episode, and same goes with polPI_list (policies)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "# visualize your final result\n",
    "%matplotlib notebook\n",
    "\n",
    "m.new_render()\n",
    "m.render(V_list[-1], polPI_list[-1]) # V_list is the list of the V values of each episode, and same goes with polPI_list (policies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "The QLearning algorithm accounts for an agent exploring an MDP and updating at each step a model of the state action-value function stored into a Q-table. It is updated as follows:\n",
    "    $$Q^{(t+1)}(x_t,u_t) = Q_{(t)}(x_t,u_t) + \\alpha \\delta_t,$$\n",
    "    \n",
    "and the temporal difference error is processed using $\\delta_t = r(x_t,u_t) + \\gamma \\max_{u_{t+1} \\in A} Q^{(t)}(x_{t+1},u_{t+1})-Q^{(t)}(x_t,u_{t})$\n",
    "\n",
    "The cell below gives the code of Q-learning, where you must just write the central update rule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "########################### Q-Learning ###########################\n",
    "    \n",
    "# Given a temperature \"tau\", the QLearning function computes the state action-value function \n",
    "# based on a softmax policy \n",
    "def QLearning(mdp,tau,nEpisodes=100000,nTimesteps=50,alpha=0.01,render=True):\n",
    "    # Initialize the state-action value function\n",
    "    # alpha is the learning rate\n",
    "    Q = np.zeros((mdp.observation_space.size,mdp.action_space.size))\n",
    "    \n",
    "    Q_list = []\n",
    "    policy_list = []\n",
    "    \n",
    "    # Run learning cycle\n",
    "    mdp.timeout = nTimesteps #episode length\n",
    "    \n",
    "    if render:\n",
    "        mdp.new_render()\n",
    "        \n",
    "    for i in range(nEpisodes) :\n",
    "        #Draw the first state of episode i using a uniform distribution over all the states\n",
    "        x = mdp.reset(uniform=True) \n",
    "        done = mdp.done()\n",
    "        while not done:\n",
    "            if render :\n",
    "                # Show the agent in the maze\n",
    "                mdp.render(Q, Q.argmax(axis=1))\n",
    "            \n",
    "            # Draw an action using a soft-max policy\n",
    "            u = mdp.action_space.sample(prob_list=softmax(Q,x,tau))\n",
    "\n",
    "            # Perform a step of the MDP\n",
    "            [y,r,done,info] = mdp.step(u)\n",
    "\n",
    "            # Update the state-action value function with Q-Learning\n",
    "            if x in mdp.terminal_states:\n",
    "                Q[x,u] = \"\\\\...\"\n",
    "            else:\n",
    "                delta = \"\\\\...\"\n",
    "                Q[x,u] = Q[x,u] + \"\\\\...\"\n",
    "            \n",
    "            # Update agent's position\n",
    "            x = y\n",
    "            \n",
    "        # Save state-action value after each episode\n",
    "        Q_list.append(Q.copy())\n",
    "        policy_list.append(Q.argmax(axis=1))\n",
    "\n",
    "    if render :\n",
    "        # Show the agent in the maze\n",
    "        mdp.render(Q, Q.argmax(axis=1))\n",
    "    return [Q_list, policy_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, write the code to run Q-learning below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"run the Q-learning code here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d420a78fa89c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTML\u001b[0m  \u001b[0;31m# used to display the video on the notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQPI_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolPI_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# QPI_list is the list of the Q values processed at each episode,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# polPi_list is the list of the policies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "# Visualize your results\n",
    "\n",
    "# In DP, the rendering was done in real time, meaning that as the Q values were being processed, the maze environment\n",
    "# was updated, even for rather large mazes. For RL algorithms, convergence is slower, \n",
    "# so we suggest you to build a much smaller maze, using for instance:\n",
    "# walls = [6, 13, 14, 15]\n",
    "# height = 4\n",
    "# width = 5\n",
    "# terminal_states=[width*height-1]\n",
    "# m = maze_mdp(width, height, walls=walls, terminal_states=terminal_states)\n",
    "\n",
    "# As output, lists of the Q values and policies are stored and can be used either to show the result offline\n",
    "# or to generate videos that you can show on the notebook or save into your disk\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "from IPython.display import HTML  # used to display the video on the notebook\n",
    "\n",
    "m.new_render()\n",
    "m.render(QPI_list[-1], polPI_list[-1])  # QPI_list is the list of the Q values processed at each episode, \n",
    "# polPi_list is the list of the policies\n",
    "\n",
    "# you can generate a video of your learning process using those lists\n",
    "# nb_frames = 100 # number of frames of your video\n",
    "# ani = m.create_animation(QPI_list, polPI_list, nb_frames) # generate your video\n",
    "# HTML(ani.to_jshtml()) # show your video on the notebook (with a video widget than enables you to manage your output)\n",
    "# ani.save(\"Q_learning.mp4\") # save your video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Learning dynamics\n",
    "    \n",
    "If you watch carefully the values while the agent is learning, you will see that the agent favors certains paths over others which have a strictly equivalent value. This can be explained easily: as the agent chooses a path for the first time, it updates the values along that path, these values get higher than the surrounding values, and the agent will choose the same path again and again, increasing the phenomenon. Only steps of random exploration can counterbalance this effect, but they do so extremely slowly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of hyper-parameters\n",
    "\n",
    "There are three hyper-parameters in Q-learning: the softmax temperature $\\tau$, the learning rate $\\alpha$, and the discount factor $\\gamma$. Using a small maze, try various values for these hyper-parameters and explain what is happenning.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration\n",
    "\n",
    "\n",
    "In the code above, action selection is based on a soft-max policy. Instead, it could have relied on *epsilon-greedy*.\n",
    "Copy-paste the above code below and do the replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"copy-paste and replace here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA\n",
    "\n",
    "The SARSA algorithm is very similar to Q_learning. At first glance, the only difference is in the update rule. However, to perform the update in SARSA, one needs to know the action the agent will take when it will be at the next state, even if the agent is taking a random action.\n",
    "\n",
    "This implies that the next state action is determined in advance and stored for being played at the next time step.\n",
    "\n",
    "\n",
    "By taking inspiration from the above Qlearning function, write in the cell below a Sarsa function that implements the corresponding algorithm. Then write the code to run it in the cell after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"write SARSA here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"run SARSA here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# visualize your results\n",
    "\n",
    "%matplotlib notebook\n",
    "\n",
    "m.new_render()\n",
    "m.render(QS_list[-1], polS_list[-1]) # output the last Q values and policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "\n",
    "Compare the number of steps needed by Qlearning and Sarsa to converge on the given MDP. To figure out, add a counter of number of steps to your various algorithms, and run them for a given number of steps (for instance, 10.000). Then watch the corresponding Q-table: can you determine if one was updated more than another? Eventually, do so with much smaller mazes..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
